from django.contrib.staticfiles.testing import LiveServerTestCase
from django.test.testcases import call_command, connection, connections
from selenium.webdriver.common.by import By
from webdriver_manager.firefox import GeckoDriverManager
from selenium import webdriver
import requests
from selenium.webdriver.firefox.service import Service
from django.db import connection
from .extra_utils.util_functions import varnish_cache
from bs4 import BeautifulSoup as BSoup

# Custom utilities
from pgweb.utils.report_generation import write_to_report

# Fix for CASCADE TRUNCATE FK error


def _fixture_teardown(self):
    # Allow TRUNCATE ... CASCADE and don't emit the post_migrate signal
    # when flushing only a subset of the apps
    for db_name in self._databases_names(include_mirrors=False):
        # Flush the database
        inhibit_post_migrate = (
            self.available_apps is not None
            or (  # Inhibit the post_migrate signal when using serialized
                # rollback to avoid trying to recreate the serialized data.
                self.serialized_rollback
                and hasattr(connections[db_name], "_test_serialized_contents")
            )
        )
        call_command(
            "flush",
            verbosity=0,
            interactive=False,
            database=db_name,
            reset_sequences=False,
            # In the real TransactionTestCase this is conditionally set to False.
            allow_cascade=True,
            inhibit_post_migrate=inhibit_post_migrate,
        )


LiveServerTestCase._fixture_teardown = _fixture_teardown
# ---------------------------

external_links = []
internal_links = []
broken_internal_links = {}
broken_external_links = {}

# Generate a list of all the urls of the website


def segregate_links(seln, addr):

    urls = [addr + "/"]
    all_urls = [addr + "/"]

    print("Url")
    print(all_urls)

    while len(urls) > 0:
        print("Checking -> ", urls[0])
        # seln.get(urls[0])
        page = requests.get(urls[0]).content
        content = BSoup(page, "html.parser")
        links = content.find_all('a')
        # print(links)
        for lk in links:
            url = lk.get('href')
            if url:
                if url.startswith('/'):
                    url = addr + url
                elif url.startswith('#'):
                    url = addr + "/" + url
                print(url)
                if not url in all_urls:
                    urls.append(url)
                    all_urls.append(url)
        del urls[0]

    for url in all_urls:
        if url.__contains__('mailto'):
            continue
        if url.__contains__("localhost") or url.startswith('/'):
            internal_links.append(url)
        else:
            external_links.append(url)

    print(all_urls)


class RecusrsiveLinkCrawlTests(LiveServerTestCase):

    fixtures = ['pgweb/core/fixtures/data.json', 'pgweb/docs/fixtures/data.json',
                'pgweb/lists/fixtures/data.json', 'pgweb/sponsors/fixtures/data.json', 'pgweb/contributors/fixtures/data.json',
                'pgweb/featurematrix/fixtures/data.json']

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        options = webdriver.FirefoxOptions()
        options.headless = True
        serv = Service(executable_path=GeckoDriverManager().install())
        cls.selenium = webdriver.Firefox(
            service=serv, options=options)

        # Calling SQL execution for varnish cache
        varnish_cache()

        # Segregation of internal and external links
        print("Segregating")
        segregate_links(cls.selenium, cls.live_server_url)

    @classmethod
    def tearDownClass(cls):
        cls.selenium.quit()
        super().tearDownClass()

    def test_external_links(self):
        print("OKK")
        self.assertTrue(True)
    #     for lnk in external_links:
    #         res = requests.get(lnk)
    #         if not res is None:
    #             stat = res.status_code
    #             if not stat == 200:
    #                 broken_external_links[lnk] = stat
    #         else:
    #             broken_external_links[lnk] = "Not reachable"
    #     print("External Links: ", external_links)
    #     # if len(broken_external_links) > 0:
    #     #     write_to_report(broken_external_links, "Broken External Urls")
    #     self.assertTrue(len(broken_external_links) ==
    #                     0, msg=broken_external_links)
    #
    # def test_internal_links(self):
    #     for lnk in internal_links:
    #         res = requests.get(lnk)
    #         if not res is None:
    #             stat = res.status_code
    #             if not stat == 200:
    #                 broken_internal_links[lnk] = stat
    #         else:
    #             broken_internal_links[lnk] = "Not reachable"
    #
    #     # Checking if the internal URL is working on the deployed version
    #     to_rem = []
    #     for lk in broken_internal_links.keys():
    #         lvk = str(lk).replace(
    #             f'{self.live_server_url}', "https://www.postgresql.org")
    #         if requests.get(lvk).status_code == 200:
    #             to_rem.append(lk)
    #
    #     # Removing false errors
    #     for working_link in to_rem:
    #         broken_internal_links.pop(working_link)
    #
    #     print("Internal Links: ", internal_links)
    #
    #     # Final List of Broken Urls
    #     # if len(broken_internal_links) > 0:
    #     # write_to_report(broken_internal_links, "Broken Internal Urls")
    #
    #     self.assertTrue(len(broken_internal_links) ==
    #                     0, msg=broken_internal_links)
